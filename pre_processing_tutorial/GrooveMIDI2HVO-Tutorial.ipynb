{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this notebook, we will import the Groove Midi Dataset and process it, and save it for easily being used in our pipelines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.enable_eager_execution() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "console.log('Starting front end url_querystring_target comm target');\n",
       "const comm = Jupyter.notebook.kernel.comm_manager.new_comm('url_querystring_target', {'init': 1});\n",
       "comm.send({'ipyparams_browser_url': window.location.href});\n",
       "console.log('Sent window.location.href on url_querystring_target comm target');\n",
       "\n",
       "comm.on_msg(function(msg) {\n",
       "    console.log(msg.content.data);\n",
       "});\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "# For some reason, tfds import gives an error on the first attempt but works second time around\n",
    "try:                              \n",
    "    import tensorflow_datasets as tfds\n",
    "except:\n",
    "    import tensorflow_datasets as tfds\n",
    "    \n",
    "# Import necessary libraries for processing/loading/storing the dataset\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import ipyparams\n",
    "from shutil import copy2\n",
    "\n",
    "# Import libraries for creating/naming folders/files\n",
    "import os, sys\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the HVO_Sequence implementation\n",
    "sys.path.append(\"../\") # --> unnecessary in future implementations (when package installed via pip)\n",
    "from hvo_sequence.io_helpers import note_sequence_to_hvo_sequence\n",
    "from hvo_sequence.drum_mappings import ROLAND_REDUCED_MAPPING\n",
    "\n",
    "# Import magenta's note_seq \n",
    "import note_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# tfds works in both Eager and Graph modes\n",
    "#tf.enable_eager_execution() #not needed in TF V2, as it is already the default\n",
    "#import magenta.music as mm \n",
    "# from magenta.models.music_vae import data   <---- DELETE\n",
    "#import magenta\n",
    "\n",
    "#print (magenta.__version__)\n",
    "#from visual_midi import Plotter\n",
    "#from pretty_midi import PrettyMIDI\n",
    "#from IPython.core.display import display, HTML\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We have the dataset already available in tensorflow_datasets, so let's load it first\n",
    "\n",
    "\n",
    "#####  NOTE:\n",
    " you can convert midi to note_sequence, two ways:\n",
    " 1. USE magenta.music.midi_to_note_sequence, BUT THIS will GIVE YOU A PICKLING ERROR\n",
    " 2. install note_seq (pip install note_seq) then use note_seq.midi_to_note_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's Start With the 2bar-midionly dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train_unprocessed,dataset_train_info = tfds.load(\n",
    "    name=\"groove/2bar-midionly\",\n",
    "    split=tfds.Split.TRAIN,\n",
    "    try_gcs=True,\n",
    "    with_info=True)\n",
    "\n",
    "dataset_test_unprocessed = tfds.load(\n",
    "    name=\"groove/2bar-midionly\",\n",
    "    split=tfds.Split.TEST,\n",
    "    try_gcs=True)\n",
    "\n",
    "dataset_validation_unprocessed = tfds.load(\n",
    "    name=\"groove/2bar-midionly\",\n",
    "    split=tfds.Split.VALIDATION,\n",
    "    try_gcs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE: TF DATASETS ARE PYTHON ITERABLES\n",
    "you can convert the dataset into a list or numpy array:\n",
    "\n",
    "   * List(dataset)\n",
    "   * tfds.as_numpy(dataset)\n",
    "    \n",
    "Lets see how many samples we've got in the dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Number of Examples in Train Set: 18163, Test Set: 2204, Validation Set: 2252\n"
     ]
    }
   ],
   "source": [
    " # In all three sets, separate entries into individual examples \n",
    "dataset_train = dataset_train_unprocessed.batch(1)\n",
    "dataset_test  = dataset_test_unprocessed.batch(1)\n",
    "dataset_validation = dataset_validation_unprocessed.batch(1)\n",
    "\n",
    "print(\"\\n Number of Examples in Train Set: {}, Test Set: {}, Validation Set: {}\".format(\n",
    "    len(list(dataset_train)), \n",
    "    len(list(dataset_test)), \n",
    "    len(list(dataset_validation)))\n",
    "     ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: ../resources: File exists\n",
      "mkdir: ../resources/source_dataset: File exists\n",
      "--2021-04-27 20:07:42--  https://storage.googleapis.com/magentadata/datasets/groove/groove-v1.0.0-midionly.zip\n",
      "Resolving storage.googleapis.com... 142.250.200.144, 142.250.184.16, 216.58.215.176, ...\n",
      "Connecting to storage.googleapis.com|142.250.200.144|:443... connected.\n",
      "HTTP request sent, awaiting response... 304 Not Modified\n",
      "File '../resources/source_dataset/groove-v1.0.0-midionly.zip' not modified on server. Omitting download.\n",
      "\n",
      "Archive:  ../resources/source_dataset/groove-v1.0.0-midionly.zip\n"
     ]
    }
   ],
   "source": [
    "# Download groovemidi set (midi files and metadata of performances, not chopped files!) \n",
    "!mkdir ../resources\n",
    "!mkdir ../resources/source_dataset\n",
    "!source activate torch_thesis; wget -N -P ../resources/source_dataset https://storage.googleapis.com/magentadata/datasets/groove/groove-v1.0.0-midionly.zip\n",
    "!unzip -n ../resources/source_dataset/groove-v1.0.0-midionly.zip -d ../resources/source_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, we will start pre-process the downloaded 2-bar groove midi datasets.\n",
    "The metadata for the dataset is not available via the tfds.load() method. As a result, I have manually downloaded the groove-v1.0.0-midionly.zip dataset from here:\n",
    "        \n",
    "        https://magenta.tensorflow.org/datasets/groove\n",
    "        \n",
    "In this folder (currently in resources/source_dataset), there is a csv file containing the metadata for each of the examples. I will match each of the 2-bar examples (downloaded from tfds.load()) with the csv file available in the manually downloaded groove-v1.0.0-midionly.zip\n",
    "\n",
    "\n",
    "To keep track of the metadata, a panda's dataframe with the following fields is used\n",
    "\n",
    "* Keys from Groove MIDI Dataset:\n",
    "    * bpm\n",
    "    * drummer\n",
    "    * id \n",
    "    * midi\n",
    "    * style\n",
    "    * time_signature\n",
    "    * type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>drummer</th>\n",
       "      <th>session</th>\n",
       "      <th>id</th>\n",
       "      <th>style</th>\n",
       "      <th>bpm</th>\n",
       "      <th>beat_type</th>\n",
       "      <th>time_signature</th>\n",
       "      <th>midi_filename</th>\n",
       "      <th>audio_filename</th>\n",
       "      <th>duration</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>drummer1</td>\n",
       "      <td>drummer1/eval_session</td>\n",
       "      <td>drummer1/eval_session/1</td>\n",
       "      <td>funk/groove1</td>\n",
       "      <td>138</td>\n",
       "      <td>beat</td>\n",
       "      <td>4-4</td>\n",
       "      <td>drummer1/eval_session/1_funk-groove1_138_beat_...</td>\n",
       "      <td>drummer1/eval_session/1_funk-groove1_138_beat_...</td>\n",
       "      <td>27.872308</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>drummer1</td>\n",
       "      <td>drummer1/eval_session</td>\n",
       "      <td>drummer1/eval_session/10</td>\n",
       "      <td>soul/groove10</td>\n",
       "      <td>102</td>\n",
       "      <td>beat</td>\n",
       "      <td>4-4</td>\n",
       "      <td>drummer1/eval_session/10_soul-groove10_102_bea...</td>\n",
       "      <td>drummer1/eval_session/10_soul-groove10_102_bea...</td>\n",
       "      <td>37.691158</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>drummer1</td>\n",
       "      <td>drummer1/eval_session</td>\n",
       "      <td>drummer1/eval_session/2</td>\n",
       "      <td>funk/groove2</td>\n",
       "      <td>105</td>\n",
       "      <td>beat</td>\n",
       "      <td>4-4</td>\n",
       "      <td>drummer1/eval_session/2_funk-groove2_105_beat_...</td>\n",
       "      <td>drummer1/eval_session/2_funk-groove2_105_beat_...</td>\n",
       "      <td>36.351218</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>drummer1</td>\n",
       "      <td>drummer1/eval_session</td>\n",
       "      <td>drummer1/eval_session/3</td>\n",
       "      <td>soul/groove3</td>\n",
       "      <td>86</td>\n",
       "      <td>beat</td>\n",
       "      <td>4-4</td>\n",
       "      <td>drummer1/eval_session/3_soul-groove3_86_beat_4...</td>\n",
       "      <td>drummer1/eval_session/3_soul-groove3_86_beat_4...</td>\n",
       "      <td>44.716543</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>drummer1</td>\n",
       "      <td>drummer1/eval_session</td>\n",
       "      <td>drummer1/eval_session/4</td>\n",
       "      <td>soul/groove4</td>\n",
       "      <td>80</td>\n",
       "      <td>beat</td>\n",
       "      <td>4-4</td>\n",
       "      <td>drummer1/eval_session/4_soul-groove4_80_beat_4...</td>\n",
       "      <td>drummer1/eval_session/4_soul-groove4_80_beat_4...</td>\n",
       "      <td>47.987500</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    drummer                session                        id          style  \\\n",
       "0  drummer1  drummer1/eval_session   drummer1/eval_session/1   funk/groove1   \n",
       "1  drummer1  drummer1/eval_session  drummer1/eval_session/10  soul/groove10   \n",
       "2  drummer1  drummer1/eval_session   drummer1/eval_session/2   funk/groove2   \n",
       "3  drummer1  drummer1/eval_session   drummer1/eval_session/3   soul/groove3   \n",
       "4  drummer1  drummer1/eval_session   drummer1/eval_session/4   soul/groove4   \n",
       "\n",
       "   bpm beat_type time_signature  \\\n",
       "0  138      beat            4-4   \n",
       "1  102      beat            4-4   \n",
       "2  105      beat            4-4   \n",
       "3   86      beat            4-4   \n",
       "4   80      beat            4-4   \n",
       "\n",
       "                                       midi_filename  \\\n",
       "0  drummer1/eval_session/1_funk-groove1_138_beat_...   \n",
       "1  drummer1/eval_session/10_soul-groove10_102_bea...   \n",
       "2  drummer1/eval_session/2_funk-groove2_105_beat_...   \n",
       "3  drummer1/eval_session/3_soul-groove3_86_beat_4...   \n",
       "4  drummer1/eval_session/4_soul-groove4_80_beat_4...   \n",
       "\n",
       "                                      audio_filename   duration split  \n",
       "0  drummer1/eval_session/1_funk-groove1_138_beat_...  27.872308  test  \n",
       "1  drummer1/eval_session/10_soul-groove10_102_bea...  37.691158  test  \n",
       "2  drummer1/eval_session/2_funk-groove2_105_beat_...  36.351218  test  \n",
       "3  drummer1/eval_session/3_soul-groove3_86_beat_4...  44.716543  test  \n",
       "4  drummer1/eval_session/4_soul-groove4_80_beat_4...  47.987500  test  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe = pd.read_csv('../resources/source_dataset/groove/info.csv', delimiter = ',')\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['drummer1/eval_session'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A quick guide to using the panda dataframe\n",
    "(dataframe[dataframe.id == \"drummer1/eval_session/1\"][\"session\"]).to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Difference between tfds.load(name=\"groove/2bar-midionly\") and groove-v1.0.0-midionly.zip\n",
    "\n",
    "In groove-v1.0.0-midionly.zip, we have access to full performances, while using tfds.load(name=\"groove/2bar-midionly\"), we can readily access the performance chopped into 2 bar segments. \n",
    "\n",
    "However, in the pre-chopped set, the meta data is missing. \n",
    "\n",
    "Hence, we need to find the relevant metadata in the info.csv file available groove-v1.0.0-midionly.zip. \n",
    "\n",
    "To do so, we match the beginning of the id in the chopped segments with the id in info.csv\n",
    "\n",
    "\n",
    "# Preprocessing Steps\n",
    "\n",
    "\n",
    "To preprocess the set, we will go through each of the examples in train/test/eval sets and do the following:\n",
    "\n",
    "    1. get the midi file from the \"midi\" field available from tfds.load()\n",
    "    2. convert midi to note_sequence \n",
    "    2. convert the note_sequence to HVO_Sequence\n",
    "    3. get the id from the \"id\" field available from tfds.load()\n",
    "    4. Match the beginning of the \"id\" field with the ids in the pandas dataframe (loaded from groove-v1.0.0-midionly.zip)\n",
    "    5. Store all processed/retrieved fields in a dictionary\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_append(dictionary, key, vals):\n",
    "    # Appends a value or a list of values to a key in a dictionary\n",
    "    \n",
    "    # if the values for a key are not a list, they are converted to a list and then extended with vals\n",
    "    dictionary[key]=list(dictionary[key]) if not isinstance(dictionary[key], list) else dictionary[key]\n",
    "    \n",
    "    # if vals is a single value (not a list), it's converted to a list so as to be iterable\n",
    "    vals = [vals] if not isinstance(vals, list) else vals\n",
    "        \n",
    "    # append new values \n",
    "    for val in vals:\n",
    "        dictionary[key].append(val)\n",
    "\n",
    "    return dictionary\n",
    "\n",
    "def convert_groove_midi_dataset(dataset, beat_division_factors=[4], csv_dataframe_info=None):\n",
    "    \n",
    "    dataset_dict_processed = dict()\n",
    "    dataset_dict_processed.update({\n",
    "        \"drummer\":[],\n",
    "        \"session\":[],\n",
    "        \"loop_id\":[],  # the id of the recording from which the loop is extracted\n",
    "        \"master_id\":[], # the id of the recording from which the loop is extracted\n",
    "        \"style_primary\":[],\n",
    "        \"style_secondary\":[],\n",
    "        \"bpm\":[],\n",
    "        \"beat_type\":[],\n",
    "        \"time_signature\":[],\n",
    "        \"full_midi_filename\":[],\n",
    "        \"full_audio_filename\":[],\n",
    "        \"midi\":[],\n",
    "        \"note_sequence\":[],\n",
    "        \"hvo_sequence\":[],\n",
    "    })\n",
    "    \n",
    "    for features in dataset:\n",
    "        \n",
    "        # Features to be extracted from the dataset\n",
    "        \n",
    "        note_sequence = note_seq.midi_to_note_sequence(tfds.as_numpy(features[\"midi\"][0]))\n",
    "        \n",
    "        \n",
    "        if note_sequence.notes: # ignore if no notes in note_sequence (i.e. empty 2 bar sequence)\n",
    "                        \n",
    "            _hvo_seq = note_sequence_to_hvo_sequence(\n",
    "                ns = note_sequence, \n",
    "                drum_mapping = ROLAND_REDUCED_MAPPING,\n",
    "                beat_division_factors = beat_division_factors\n",
    "            )\n",
    "            \n",
    "            if not csv_dataframe_info.empty:\n",
    "\n",
    "                # Master ID for the Loop\n",
    "                main_id = features[\"id\"].numpy()[0].decode(\"utf-8\").split(\":\")[0]\n",
    "\n",
    "                # Get the relevant series from the dataframe\n",
    "                df = csv_dataframe_info[csv_dataframe_info.id == main_id]\n",
    "                \n",
    "                # Update the dictionary associated with the metadata\n",
    "                dict_append(dataset_dict_processed, \"drummer\", df[\"drummer\"].to_numpy()[0])\n",
    "                dict_append(dataset_dict_processed, \"session\", df[\"session\"].to_numpy()[0].split(\"/\")[-1])\n",
    "                dict_append(dataset_dict_processed, \"loop_id\", features[\"id\"].numpy()[0].decode(\"utf-8\"))\n",
    "                dict_append(dataset_dict_processed, \"master_id\", main_id)\n",
    "\n",
    "                style_full = df[\"style\"].to_numpy()[0]\n",
    "                style_primary = style_full.split(\"/\")[0]\n",
    "                dict_append(dataset_dict_processed, \"style_primary\", style_primary)\n",
    "\n",
    "                if \"/\" in style_full:\n",
    "                    style_secondary = style_full.split(\"/\")[1]\n",
    "                    dict_append(dataset_dict_processed, \"style_secondary\", style_secondary)\n",
    "                else:\n",
    "                    dict_append(dataset_dict_processed, \"style_secondary\", [\"None\"])\n",
    "\n",
    "                dict_append(dataset_dict_processed, \"bpm\", df[\"bpm\"].to_numpy()[0])\n",
    "                dict_append(dataset_dict_processed, \"beat_type\", df[\"beat_type\"].to_numpy()[0])\n",
    "                dict_append(dataset_dict_processed, \"time_signature\", df[\"time_signature\"].to_numpy()[0])\n",
    "                dict_append(dataset_dict_processed, \"full_midi_filename\", df[\"midi_filename\"].to_numpy()[0])\n",
    "                dict_append(dataset_dict_processed, \"full_audio_filename\", df[\"audio_filename\"].to_numpy()[0])\n",
    "\n",
    "                dict_append(dataset_dict_processed, \"midi\", features[\"midi\"].numpy()[0])\n",
    "                dict_append(dataset_dict_processed, \"note_sequence\", [note_sequence])\n",
    "                        \n",
    "            dict_append(dataset_dict_processed, \"hvo_sequence\", _hvo_seq)\n",
    "\n",
    "        else:\n",
    "            pass \n",
    "            \n",
    "    return dataset_dict_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 10s, sys: 2.13 s, total: 1min 12s\n",
      "Wall time: 1min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Process Training Set\n",
    "dataset_train = dataset_train_unprocessed.batch(1)\n",
    "dataset_train_processed = convert_groove_midi_dataset(\n",
    "    dataset = dataset_train, \n",
    "    beat_division_factors=[4], \n",
    "    csv_dataframe_info=dataframe\n",
    ")\n",
    "\n",
    "# Process Test Set\n",
    "dataset_test = dataset_test_unprocessed.batch(1)\n",
    "dataset_test_processed = convert_groove_midi_dataset(\n",
    "    dataset = dataset_test, \n",
    "    beat_division_factors=[4], \n",
    "    csv_dataframe_info=dataframe)\n",
    "\n",
    "# Process Validation Set\n",
    "dataset_validation = dataset_validation_unprocessed.batch(1)\n",
    "dataset_validation_processed = convert_groove_midi_dataset(\n",
    "    dataset = dataset_validation, \n",
    "    beat_division_factors=[4], \n",
    "    csv_dataframe_info=dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Order the pre-processed set\n",
    "The batching process shuffles the dataset. However, we prefer to store the dataset in sequential order. Hence, we need to manually order the dataset. In this tutorial, I order the sequences by loop_id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_dictionary_by_key (dictionary_to_sort, key_used_to_sort):\n",
    "    # sorts a dictionary according to the list within a given key\n",
    "    sorted_ids=np.argsort(dictionary_to_sort[key_used_to_sort])\n",
    "    for key in dictionary_to_sort.keys():\n",
    "        dictionary_to_sort[key]=[dictionary_to_sort[key][i] for i in sorted_ids]\n",
    "    return dictionary_to_sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the sets using ids\n",
    "dataset_train_processed = sort_dictionary_by_key(dataset_train_processed, \"loop_id\")\n",
    "dataset_test_processed = sort_dictionary_by_key(dataset_test_processed, \"loop_id\")\n",
    "dataset_validation_processed = sort_dictionary_by_key(dataset_validation_processed, \"loop_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------\n",
    "--------------------------------------\n",
    "\n",
    "# Storing Pre-Processed Sets\n",
    "\n",
    "The folder structure is as follows:\n",
    "\n",
    "../processed_dataset\n",
    "\n",
    "      |\n",
    "      |--> /Processed_On_%d_%m_%Y_at_%H_%M_hrs\n",
    "            |\n",
    "            |--> /GrooveMIDI_processed_{train, test, or validation}\n",
    "                    |\n",
    "                    |--> hvo_sequence.obj\n",
    "                    |\n",
    "                    |--> note_sequence.obj\n",
    "                    |\n",
    "                    |--> midi.obj\n",
    "                    |\n",
    "                    |--> midi.obj\n",
    "                    |\n",
    "                    |--> metadata.csv\n",
    "                    \n",
    "                    \n",
    "The midi files, hvo_sequences and note_sequences along with corresponding metadata are all saved in separate files. The order of entries in all these files match one another (increasing in alpha-numeric order of \"loopid\"s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DUMP INTO A PICKLE FILE\n",
    "def store_dataset_as_pickle(dataset_list, \n",
    "                            filename_list, \n",
    "                            root_dir = \"../processed_dataset\",\n",
    "                            append_datetime=True, \n",
    "                            features_with_separate_picklefile = [\"hvo\", \"midi\", \"note_seq\"]\n",
    "                           ):\n",
    "\n",
    "    #filename = filename.split(\".obj\")[0]\n",
    "    path = root_dir\n",
    "    \n",
    "    if append_datetime:\n",
    "        now = datetime.now()\n",
    "        dt_string = now.strftime(\"%d_%m_%Y_at_%H_%M_hrs\")\n",
    "    else:\n",
    "        dt_string =\"\"\n",
    "        \n",
    "    path = os.path.join(path, \"Processed_On_\"+dt_string)\n",
    "    \n",
    "    if not os.path.exists (path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "    currentNotebook = ipyparams.notebook_name\n",
    "    print(\"Copying Source Code from %s to %s\" % (os.path.join(os.getcwd(), currentNotebook), path))\n",
    "    print(\"-\"*100)\n",
    "\n",
    "    # copy2(os.path.join(os.getcwd(), currentNotebook), path) \n",
    "    \n",
    "    for i, dataset in enumerate(dataset_list):\n",
    "\n",
    "        subdirectory = os.path.join(path, filename_list[i])\n",
    "        if not os.path.exists (subdirectory):\n",
    "            os.makedirs(subdirectory)\n",
    "            \n",
    "        print(\"-\"*100)\n",
    "        print(\"-\"*100)\n",
    "        print(\"Processing %s folder\" % subdirectory)\n",
    "        print(\"-\"*100)\n",
    "        print(\"-\"*100)\n",
    "        \n",
    "        # Create Metadata File\n",
    "        csv_dataframe = pd.DataFrame()\n",
    "        \n",
    "        for k in dataset.keys():\n",
    "            if k not in features_with_separate_picklefile:\n",
    "                csv_dataframe[k] = dataset[k]\n",
    "        \n",
    "        csv_dataframe.to_csv(os.path.join(subdirectory, \"metadata.csv\"))\n",
    "        \n",
    "        print(\"Metadata created!\")\n",
    "        print(\"-\"*100)\n",
    "\n",
    "        for feature in features_with_separate_picklefile:\n",
    "            if feature in dataset.keys():\n",
    "                dataset_filehandler = open(os.path.join(subdirectory, \"%s_data.obj\"%feature),\"wb\")\n",
    "                print(feature)\n",
    "                print(dataset_filehandler)\n",
    "                pickle.dump(dataset[feature],  dataset_filehandler)\n",
    "                dataset_filehandler.close()\n",
    "                print(\"feature %s pickled at %s\" % (feature, os.path.join(subdirectory, \"%s.obj\"%filename_list[i].split(\".\")[0])))\n",
    "                print(\"-\"*100)\n",
    "\n",
    "            else:\n",
    "                 raise Warning(\"Feature is not available: \", feature)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying Source Code from /Users/pelinski/tpr/smc/tfm/code/GrooveMIDI2HVO_PreProcessing/pre_processing_tutorial/GrooveMIDI2HVO-Tutorial.ipynb to ../processed_dataset/Processed_On_27_04_2021_at_20_09_hrs\n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Processing ../processed_dataset/Processed_On_27_04_2021_at_20_09_hrs/GrooveMIDI_processed_train folder\n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Metadata created!\n",
      "----------------------------------------------------------------------------------------------------\n",
      "hvo_sequence\n",
      "<_io.BufferedWriter name='../processed_dataset/Processed_On_27_04_2021_at_20_09_hrs/GrooveMIDI_processed_train/hvo_sequence_data.obj'>\n",
      "feature hvo_sequence pickled at ../processed_dataset/Processed_On_27_04_2021_at_20_09_hrs/GrooveMIDI_processed_train/GrooveMIDI_processed_train.obj\n",
      "----------------------------------------------------------------------------------------------------\n",
      "midi\n",
      "<_io.BufferedWriter name='../processed_dataset/Processed_On_27_04_2021_at_20_09_hrs/GrooveMIDI_processed_train/midi_data.obj'>\n",
      "feature midi pickled at ../processed_dataset/Processed_On_27_04_2021_at_20_09_hrs/GrooveMIDI_processed_train/GrooveMIDI_processed_train.obj\n",
      "----------------------------------------------------------------------------------------------------\n",
      "note_sequence\n",
      "<_io.BufferedWriter name='../processed_dataset/Processed_On_27_04_2021_at_20_09_hrs/GrooveMIDI_processed_train/note_sequence_data.obj'>\n",
      "feature note_sequence pickled at ../processed_dataset/Processed_On_27_04_2021_at_20_09_hrs/GrooveMIDI_processed_train/GrooveMIDI_processed_train.obj\n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Processing ../processed_dataset/Processed_On_27_04_2021_at_20_09_hrs/GrooveMIDI_processed_test folder\n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Metadata created!\n",
      "----------------------------------------------------------------------------------------------------\n",
      "hvo_sequence\n",
      "<_io.BufferedWriter name='../processed_dataset/Processed_On_27_04_2021_at_20_09_hrs/GrooveMIDI_processed_test/hvo_sequence_data.obj'>\n",
      "feature hvo_sequence pickled at ../processed_dataset/Processed_On_27_04_2021_at_20_09_hrs/GrooveMIDI_processed_test/GrooveMIDI_processed_test.obj\n",
      "----------------------------------------------------------------------------------------------------\n",
      "midi\n",
      "<_io.BufferedWriter name='../processed_dataset/Processed_On_27_04_2021_at_20_09_hrs/GrooveMIDI_processed_test/midi_data.obj'>\n",
      "feature midi pickled at ../processed_dataset/Processed_On_27_04_2021_at_20_09_hrs/GrooveMIDI_processed_test/GrooveMIDI_processed_test.obj\n",
      "----------------------------------------------------------------------------------------------------\n",
      "note_sequence\n",
      "<_io.BufferedWriter name='../processed_dataset/Processed_On_27_04_2021_at_20_09_hrs/GrooveMIDI_processed_test/note_sequence_data.obj'>\n",
      "feature note_sequence pickled at ../processed_dataset/Processed_On_27_04_2021_at_20_09_hrs/GrooveMIDI_processed_test/GrooveMIDI_processed_test.obj\n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Processing ../processed_dataset/Processed_On_27_04_2021_at_20_09_hrs/GrooveMIDI_processed_validation folder\n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Metadata created!\n",
      "----------------------------------------------------------------------------------------------------\n",
      "hvo_sequence\n",
      "<_io.BufferedWriter name='../processed_dataset/Processed_On_27_04_2021_at_20_09_hrs/GrooveMIDI_processed_validation/hvo_sequence_data.obj'>\n",
      "feature hvo_sequence pickled at ../processed_dataset/Processed_On_27_04_2021_at_20_09_hrs/GrooveMIDI_processed_validation/GrooveMIDI_processed_validation.obj\n",
      "----------------------------------------------------------------------------------------------------\n",
      "midi\n",
      "<_io.BufferedWriter name='../processed_dataset/Processed_On_27_04_2021_at_20_09_hrs/GrooveMIDI_processed_validation/midi_data.obj'>\n",
      "feature midi pickled at ../processed_dataset/Processed_On_27_04_2021_at_20_09_hrs/GrooveMIDI_processed_validation/GrooveMIDI_processed_validation.obj\n",
      "----------------------------------------------------------------------------------------------------\n",
      "note_sequence\n",
      "<_io.BufferedWriter name='../processed_dataset/Processed_On_27_04_2021_at_20_09_hrs/GrooveMIDI_processed_validation/note_sequence_data.obj'>\n",
      "feature note_sequence pickled at ../processed_dataset/Processed_On_27_04_2021_at_20_09_hrs/GrooveMIDI_processed_validation/GrooveMIDI_processed_validation.obj\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "dataset_list = [dataset_train_processed,\n",
    "               dataset_test_processed,\n",
    "               dataset_validation_processed]\n",
    "\n",
    "filename_list = [\"GrooveMIDI_processed_train\",\n",
    "                \"GrooveMIDI_processed_test\",\n",
    "                \"GrooveMIDI_processed_validation\"]\n",
    "\n",
    "store_dataset_as_pickle(dataset_list, \n",
    "                        filename_list,\n",
    "                        root_dir=\"../processed_dataset\",\n",
    "                        append_datetime=True,\n",
    "                        features_with_separate_picklefile = [\"hvo_sequence\", \"midi\", \"note_sequence\"]\n",
    "                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------\n",
    "----------\n",
    "\n",
    "### Using Pre-Processed Sets After Storing\n",
    "\n",
    "Now, we can import the pre-processed sets easily into our scripts without going through this procedure. \n",
    "\n",
    "To do so, load the pickle files and the metadata from corresponding subfolders!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../processed_dataset/Processed_On_27_04_2021_at_19_04_hrs/GrooveMIDI_processed_train/hvo_data.obj\n",
      "Dataset Size: 17108 \n",
      " Features:  ['Unnamed: 0', 'drummer', 'session', 'loop_id', 'master_id', 'style_primary', 'style_secondary', 'bpm', 'beat_type', 'time_signature', 'full_midi_filename', 'full_audio_filename']\n"
     ]
    }
   ],
   "source": [
    "source_path = \"../processed_dataset/Processed_On_27_04_2021_at_19_04_hrs\"\n",
    "print(os.path.join(source_path, \"GrooveMIDI_processed_train\", \"hvo_data.obj\"))\n",
    "train_file = open(os.path.join(source_path, \"GrooveMIDI_processed_train\", \"hvo_sequence_data.obj\"),'rb')\n",
    "train_set = pickle.load(train_file)\n",
    "metadata = pd.read_csv(os.path.join(source_path, \"GrooveMIDI_processed_train\", \"metadata.csv\"))\n",
    "\n",
    "features_in_metadata = list(metadata.columns)\n",
    "\n",
    "dataset_size = len(train_set)\n",
    "\n",
    "print(\"Dataset Size: %d \\n Features: \" % dataset_size, features_in_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's look at a random entry!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Number: 5973, Primary Style: None, Secondary Style: rock\n"
     ]
    }
   ],
   "source": [
    "ix =  int(np.random.random_sample()*dataset_size)\n",
    "print(\"Sample Number: %d, Primary Style: %s, Secondary Style: %s\" % (ix, \n",
    "                                                                     metadata[\"style_secondary\"][ix], \n",
    "                                                                     metadata[\"style_primary\"][ix])\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.plotting import show\n",
    "fig = train_set[ix].to_html_plot(filename=\"../pre_processing_tutorial/misc/temp.html\",show_figure=False)\n",
    "show(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , ..., 0.00085397, 0.        ,\n",
       "       0.        ])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[ix].save_audio(filename=\"../pre_processing_tutorial/misc/temp.wav\", sf_path=\"../hvo_sequence/soundfonts/Standard_Drum_Kit.sf2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_thesis",
   "language": "python",
   "name": "torch_thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
